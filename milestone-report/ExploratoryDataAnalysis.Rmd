---
title: "SwiftKey NLP Milestone Report"
author: "Yanal Kashou"
date: "19 November 2016"
output: 
    html_document:
        theme: cerulean
        highlight: tango
        keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Project Overview
This is the milestone report for the Coursera Data Science Capstone Project, in which we aim to implement an algorithm to predict the next word, given raw text data that needs to be cleaned and analysed. The topic of our report is **NLP (Natural Language Processing)**.

# Reading in The Data
```{r data loading, message=FALSE, warning=FALSE, include=FALSE}
### Load necessary libraries
#library(openNLP)
#library(NLP)
library(tm)
library(quanteda)
library(SnowballC)

### Set seed for reproducibility
set.seed(0)

### Set working directory (Raw Data - Windows OS)
setwd("C://Users//Yanal Kashou//Data Science//Projects//R//DataScienceCapstone//data//raw_data")

# Open file connections
con1 <- file("en_US.twitter.txt", "r") 
con2 <- file("en_US.blogs.txt", "r")
con3 <- file("en_US.news.txt", "r")

RawBlogsText <- readLines(con1)
RawNewsText <- readLines(con2)
RawTwitterText <- readLines(con3)

LengthOfBlogsText <- length(RawBlogsText)
LengthOfNewsText <- length(RawNewsText)
LengthOfTwitterText <- length(RawTwitterText)

CharLengthOfBlogsText <- sum(nchar(RawBlogsText))
CharLengthOfNewsText <- sum(nchar(RawNewsText))
CharLengthOfTwitterText <- sum(nchar(RawTwitterText))

WordData <- data.frame("Line Count" = c(LengthOfBlogsText, LengthOfNewsText, LengthOfTwitterText), "Word Count" = c(0, 0, 0), "Character Count" = c(CharLengthOfBlogsText, CharLengthOfNewsText,  CharLengthOfTwitterText))


# Close file connections
close(con1, con2, con3)
```
# Exploratory Analysis and Visualization

## Textual Analysis
```{r exploratory data, echo=TRUE}
WordData
```
## Size Reduction


# Data Sampling & Cleaning
```{r sampling, echo=TRUE, message=FALSE, warning=FALSE}
# Sample 15% of the data
SampledTwitterText <- RawTwitterText[rbinom(LengthOfTwitterText*.15, LengthOfTwitterText, .5)]
SampledBlogsText <- RawBlogsText[rbinom(LengthOfBlogsText*.15, LengthOfBlogsText, .5)]
SampledNewsText <- RawNewsText[rbinom(LengthOfNewsText*.15, LengthOfNewsText, .5)]
```

```{r, message=FALSE, warning=FALSE, include=FALSE}

### Set working directory (processed_data)
setwd("C://Users//Yanal Kashou//Data Science//Projects//R//DataScienceCapstone//data//processed_data")

# Export sampled text to .txt files
con4 <- file("sampled.twitter.txt")
writeLines(SampledTwitterText, con4)
con5 <- file("sampled.blogs.txt")
writeLines(SampledBlogsText, con5)
con6 <- file("sampled.news.txt")
writeLines(SampledNewsText, con6)
close(con4, con5, con6)

### Create Corpus 
TextCorpus <- Corpus(DirSource(directory = getwd(), pattern="sampled.*.txt|sampled.*.txt"))
inspect(TextCorpus)
str(TextCorpus)

```



```{r, message=FALSE, warning=FALSE, include=FALSE}
### Set working directory (processed_data)
setwd("C://Users//Yanal Kashou//Data Science//Projects//R//DataScienceCapstone//data//processed_data")
```
```{r cleaning and preprocessing, echo=TRUE, message=FALSE, warning=FALSE}
# Remove punctuation, whitespace, numbers, stopwords from text and convert to lower case 
inspect(TextCorpus)
CleanCorpus <- tm_map(TextCorpus, removePunctuation)
CleanCorpus <- tm_map(CleanCorpus, stripWhitespace)
CleanCorpus <- tm_map(CleanCorpus, removeNumbers)
CleanCorpus <- tm_map(CleanCorpus, removeWords, stopwords("english"))
CleanCorpus <- tm_map(CleanCorpus, content_transformer(tolower))
inspect(CleanCorpus)
```
```{r, message=FALSE, warning=FALSE, include=FALSE}
writeLines(as.character(CleanCorpus[[1]]), con="cleaned.twitter.txt")
writeLines(as.character(CleanCorpus[[2]]), con="cleaned.blogs.txt")
writeLines(as.character(CleanCorpus[[3]]), con="cleaned.news.txt")
```

# Remove Profanity
```{r trim whitespace function, message=FALSE, warning=FALSE, include=FALSE}
### Set working directory (Banned Words - Windows OS)
setwd("C://Users//Yanal Kashou//Data Science//Projects//R//DataScienceCapstone//data//banned_words")

# Open file connection
con7 <- file("banned-words.txt", "r")
BannedWords <- readLines(con7)

# Close file connection
close(con7)
# Trim whitespace at the end of the word
trim.ending <- function (x)
        sub("\\s+$", "", x)
trim.ending(BannedWords)


PoliteCorpus <- tm_map(CleanCorpus, removeWords, BannedWords)
inspect(PoliteCorpus)

writeLines(as.character(PoliteCorpus[[1]]), con="polite.twitter.txt")
writeLines(as.character(PoliteCorpus[[2]]), con="polite.blogs.txt")
writeLines(as.character(PoliteCorpus[[3]]), con="polite.news.txt")

# Create char length dataframe
#"Post-Stemming" = c(21415989, 1564823, 17116878)

CharLengthDF <- data.frame("Original" = c(31521201, 2248401, 24324928), "Post-Cleaning" = c(23806006, 1750801, 18672851), "Post-Profanity-Removal" = c(21385469, 1563741, 17030475))

# Divide char length by 10^6 to appear in millions
CharLengthDF <- CharLengthDF[1:3, ]/10^6
View(CharLengthDF)



# Final Corpus and export to txt
FinalCorpus <- tm_map(PoliteCorpus, PlainTextDocument)

writeLines(as.character(FinalCorpus[[1]]), con="final.twitter.txt")
writeLines(as.character(FinalCorpus[[2]]), con="final.blogs.txt")
writeLines(as.character(FinalCorpus[[3]]), con="final.news.txt")

FinalTwitterText <- readLines(con = "final.twitter.txt")
FinalBlogsText <- readLines(con = "final.blogs.txt")
FinalNewsText <- readLines(con = "final.news.txt")

# Document Term Matrix
TwitterDFM <- dfm(FinalTwitterText, stem = TRUE)
BlogsDFM <- dfm(FinalBlogsText, stem = TRUE)
NewsDFM <- dfm(FinalTwitterText, stem = TRUE)

# N-Grams

# Further Plans

* Test algorithms for computation efficiency (speed) and accuracy
* Attempt at the implementation of an ANN (Artificial Neural Network)
* Deploy an app using the implemented algorithm to shinyapps.io