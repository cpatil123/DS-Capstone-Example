---
title: "MilestoneReport"
author: "Yanal Kashou"
date: "November 20, 2016"
output: 
    html_document:
        theme: cerulean
        highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```
```{r libraries, message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
library(tm)
library(quanteda)
library(SnowballC)
library(ggplot2)
library(gridExtra)
library(reshape2)
library(stringi)
```

### Project Overview
This is the milestone report for the Coursera Data Science Capstone Project, in which we aim to implement an algorithm to predict the next word, given raw text data that needs to be cleaned and analysed.  
The topic of our report is **NLP (Natural Language Processing)**.

----------------------------------------------------------------------------------------

### Our Data
#### Data Loading  

We are required to load three large text files into the `R Workspace`, explore these files and clean them, before we begin with the processing of their content.  


<center>![Step 1](step1.png)</center>

```{r WordLineChar Table, echo=FALSE, message=FALSE, warning=FALSE}
setwd("C://Users//Yanal Kashou//Data Science//Projects//R//DataScienceCapstone//cache")
load("r.df.RData")
kable(r.df)
```

#### Data Sampling and Cleaning

To make it easier for computation, we have to reduce the size of the data to an acceptable value, in this case we will take a random binomial sample of 15% of each dataset to work with, clean, analyze and visualize.  

Outlined below are the pre-processing steps we walked through:

<center>![Step 2](step2.png)</center>

```{r Sampled WordLineChar Table, echo=FALSE, message=FALSE, warning=FALSE}
setwd("C://Users//Yanal Kashou//Data Science//Projects//R//DataScienceCapstone//cache")
load("s.df.RData")
kable(s.df)  
```
```{r plots, message=FALSE, warning=FALSE, include=FALSE}
s.df.long <- melt(s.df)
line.subset <- subset(s.df.long, variable == "Line Count")
```
We used the `tm` package to perform these steps and we left the stemming procedure to be performed automatically as we created a **Document Feature Matrix** using the `quanteda` package for each document in the corpus. This was done because stemming using the `tm` package caused word content to be ruined and require a further process called `stemCompletion`.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
setwd("C://Users//Yanal Kashou//Data Science//Projects//R//DataScienceCapstone//cache")
load("f.df.RData")
kable(f.df)
```

----------------------------------------------------------------------------------------

### N-Grams 
We used `NGramTokenizer` from the `RWeka` package to find the most frequent **unigrams**, **bigrams**, **trigrams** and **fourgrams**; also to produce the values below:

<center>![Step 3](Step3.png)</center>  


<center>![N-Grams Plot](Rplot02.png)</center>
----------------------------------------------------------------------------------------

### Further Plans  
#### Algorithm

<center>![Step 4](step4.png)</center>

* Test algorithms for computation efficiency (speed) and accuracy  
* Attempt at the implementation of an ANN (Artificial Neural Network)  
  
----------------------------------------------------------------------------------------

#### Data Product
  
<center>![Step 5](step5.png)</center>

* Deploy an app using the implemented algorithm to shinyapps.io

----------------------------------------------------------------------------------------
### Source Code
For access to my GitHub Repository associated with this project, please visit the URL below:  
[Data Science Capstone](https://github.com/ykashou92/DataScienceCapstone) 
